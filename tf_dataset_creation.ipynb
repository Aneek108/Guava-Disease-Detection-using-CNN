{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ9oGPtQMfr9"
   },
   "source": [
    "# Some global directories and resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rD-H_y-VMlHy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = \"E:/Gauava Disease Detection/Guava_Dataset/GuavaDiseaseDataset\"\n",
    "os.chdir(dataset_path)\n",
    "\n",
    "train_dir = os.path.join(dataset_path, \"train\")\n",
    "val_dir = os.path.join(dataset_path, \"val\")\n",
    "test_dir = os.path.join(dataset_path, \"test\")\n",
    "dataset_dirs = [train_dir, test_dir, val_dir]\n",
    "dataset_subdirs_names = [\"Anthracnose\", \"fruit_fly\", \"healthy_guava\"]\n",
    "\n",
    "dataset_subdirs = [] # Create a list for storing Subdirectories like './train/healthy_guava' for later use\n",
    "for dir in dataset_dirs:\n",
    "    for subdir_name in dataset_subdirs_names:\n",
    "        subdir = os.path.join(dir, subdir_name)\n",
    "        dataset_subdirs.append(subdir)\n",
    "\n",
    "RANDOM_SEED_1 = 42  # Used for random shuffling and sampling\n",
    "RANDOM_SEED_2 = 101 # Used for other purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/Gauava Disease Detection/Guava_Dataset/GuavaDiseaseDataset\\train\\Anthracnose: 1080\n",
      "E:/Gauava Disease Detection/Guava_Dataset/GuavaDiseaseDataset\\train\\fruit_fly: 918\n",
      "E:/Gauava Disease Detection/Guava_Dataset/GuavaDiseaseDataset\\train\\healthy_guava: 649\n",
      "E:/Gauava Disease Detection/Guava_Dataset/GuavaDiseaseDataset\\test\\Anthracnose: 156\n",
      "E:/Gauava Disease Detection/Guava_Dataset/GuavaDiseaseDataset\\test\\fruit_fly: 132\n",
      "E:/Gauava Disease Detection/Guava_Dataset/GuavaDiseaseDataset\\test\\healthy_guava: 94\n",
      "E:/Gauava Disease Detection/Guava_Dataset/GuavaDiseaseDataset\\val\\Anthracnose: 308\n",
      "E:/Gauava Disease Detection/Guava_Dataset/GuavaDiseaseDataset\\val\\fruit_fly: 262\n",
      "E:/Gauava Disease Detection/Guava_Dataset/GuavaDiseaseDataset\\val\\healthy_guava: 185\n"
     ]
    }
   ],
   "source": [
    "for subdir in dataset_subdirs:\n",
    "    print(f\"{subdir}: {len(os.listdir(subdir))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **➡ TensorFlow Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2647 files belonging to 3 classes.\n",
      "Found 382 files belonging to 3 classes.\n",
      "Found 755 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "## Creating the initial datasets from directory\n",
    "params = dict(\n",
    "    labels=\"inferred\",          # default\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=[\"Anthracnose\", \"fruit_fly\", \"healthy_guava\"],\n",
    "    color_mode='rgb',           # default\n",
    "    batch_size=32,              # default\n",
    "    image_size=(224, 224),      # default\n",
    "    shuffle=True,               # default\n",
    "    seed=RANDOM_SEED_2,\n",
    "    validation_split=None,      # default\n",
    "    subset=None,                # default\n",
    "    interpolation=\"bilinear\",   # default\n",
    "    follow_links=False,         # default\n",
    "    crop_to_aspect_ratio=False, # default\n",
    "    pad_to_aspect_ratio=False,  # default\n",
    "    data_format=None,           # default\n",
    "    verbose=True,               # default\n",
    ")\n",
    "\n",
    "train_ds = image_dataset_from_directory(directory=train_dir, **params)\n",
    "test_ds = image_dataset_from_directory(directory=test_dir, **params)\n",
    "val_ds = image_dataset_from_directory(directory=val_dir, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Split:\n",
    "- 70% - Train\n",
    "- 10% - Test\n",
    "- 20% - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>\n",
      "Total Filenames: 2647\n",
      "Classes: ['Anthracnose', 'fruit_fly', 'healthy_guava']\n",
      "Cardinality: 83 \n",
      "\n",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>\n",
      "Total Filenames: 382\n",
      "Classes: ['Anthracnose', 'fruit_fly', 'healthy_guava']\n",
      "Cardinality: 12 \n",
      "\n",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>\n",
      "Total Filenames: 755\n",
      "Classes: ['Anthracnose', 'fruit_fly', 'healthy_guava']\n",
      "Cardinality: 24 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ds in [train_ds, test_ds, val_ds]:\n",
    "    print(ds)\n",
    "    print(\"Total Filenames:\", len(ds.file_paths))\n",
    "    print(\"Classes:\", ds.class_names)\n",
    "    print(\"Cardinality:\", ds.cardinality().numpy(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upxirnFVrneq"
   },
   "source": [
    "# **➡ Saving the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sa6QIvSmkV0v"
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join(dataset_path, \"Saved_datasets\")\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFa0C57emKC4"
   },
   "source": [
    "Saving the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnmMKFSAmOj0"
   },
   "outputs": [],
   "source": [
    "save_path_subdir = os.path.join(save_path, 'train')\n",
    "os.makedirs(save_path_subdir, exist_ok=True)\n",
    "\n",
    "num_shards = 5\n",
    "\n",
    "# Sharding the train dataset into 15 shards and saving each of them in dedicated subdirectories\n",
    "\n",
    "# Uncomment the codelines below to execute again. Proceed with caution.\n",
    "# for i in range(num_shards):\n",
    "#     shard_dataset = train_ds.shard(num_shards=num_shards, index=i)\n",
    "#     shard_dataset.save(os.path.join(save_path_subdir, f'shard_{i}'), compression='GZIP')\n",
    "\n",
    "# Please comment the lines again, once runned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcABgue9lGKb"
   },
   "source": [
    "Saving the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_tP3OugjtdS"
   },
   "outputs": [],
   "source": [
    "save_path_subdir = os.path.join(save_path, 'val')\n",
    "os.makedirs(save_path_subdir, exist_ok=True)\n",
    "\n",
    "# Uncomment the codelines below to execute again. Proceed with caution.\n",
    "# val_ds.save(save_path_subdir, compression='GZIP')\n",
    "\n",
    "# Please comment the lines again, once runned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFPU3ob0lKRE"
   },
   "source": [
    "Saving the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6o9SlsgJlDdU"
   },
   "outputs": [],
   "source": [
    "save_path_subdir = os.path.join(save_path, 'test')\n",
    "os.makedirs(save_path_subdir, exist_ok=True)\n",
    "\n",
    "# Uncomment the codelines below to execute again. Proceed with caution.\n",
    "# test_ds.save(save_path_subdir, compression='GZIP')\n",
    "\n",
    "# Please comment the lines again, once runned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYFHUO6qLMba"
   },
   "source": [
    "Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O1k6XO3rKbIT"
   },
   "outputs": [],
   "source": [
    "save_path_subdir = os.path.join(save_path, 'train')\n",
    "shards = []\n",
    "for i in range(num_shards):\n",
    "    shard_path = os.path.join(save_path_subdir, f'shard_{i}')\n",
    "    shard_dataset = tf.data.Dataset.load(shard_path, compression='GZIP')\n",
    "    shards.append(shard_dataset)\n",
    "\n",
    "combined_dataset = shards[0]\n",
    "for shard in shards[1:]:\n",
    "    combined_dataset = combined_dataset.concatenate(shard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLzBqlBtK3nK",
    "outputId": "bb6f1298-04f7-4da7-9f48-81661c746b7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(83)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.cardinality().numpy()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "a1-Be3zw1dTK",
    "aQ9oGPtQMfr9",
    "wvPRqIVuEc8o",
    "oaz0IN1aEs4j",
    "ucEqVq3r1m_p",
    "D_NcwnZTMECH",
    "zL2B34kf4pNW",
    "X1lT5JUODPH7",
    "t5l5jISslS1P",
    "odPdTaEPWwQz",
    "zrycKyONhyE-",
    "VN0jIwIaGyrq",
    "G272OFSVL3XU",
    "IxYD3xOEEJCn",
    "aq5fYKneZx5w",
    "DdH_5DXVPg4f",
    "LJCraPbtX78h",
    "cBJ6M7hHFcmt",
    "dpEFO4l23-3r",
    "zkWpNO5swzYS",
    "jlw2g3jgw462",
    "4R-cs6giKaRT",
    "9WY8fi9KLOII",
    "A2cwUFs7KNbg",
    "FDZcNK7alXWT"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
